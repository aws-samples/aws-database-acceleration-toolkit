{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Welcome to Database Acceleration Toolkit(DAT) !</p> <p>The AWS Database Acceleration Toolkit(DAT) is an open-source Infrastructure-as-a-code based single click solution to simplify and automate initial setup, provisioning and on-going maintenance for Amazon Aurora database on AWS Cloud. </p> <p>It's designed to minimize the heavy lifting required for AWS customers to migrate from commercial databases to Amazon Aurora database and operating these databases in production.</p> <p>Key Features</p> <p>Key features of DAT include automation of initial Amazon Aurora setup, provisioning and on-going maintenance activities,</p> <ol> <li>Provisioning of new Amazon Aurora database cluster</li> <li>Provisioning and Integration with RDS Proxy to reuse database connections and improved reliability</li> <li>Provisioning of new Aurora Global Database</li> <li>Monitoring Aurora database </li> <li>Restore cluster from Snapshot </li> </ol> <p>Architecture </p> <p>Below is the high level architectre of Database Acceleration Toolkit.  </p>"},{"location":"contributing/","title":"Get Involved","text":"<p>Contributing Guidelines</p> <p>Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community.</p> <p>Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.</p> <p>Reporting Bugs/Feature Requests</p> <p>We welcome you to use the GitHub issue tracker to report bugs or suggest features.</p> <p>When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful:</p> <ul> <li>A reproducible test case or series of steps</li> <li>The version of our code being used</li> <li>Any modifications you've made relevant to the bug</li> <li>Anything unusual about your environment or deployment</li> </ul> <p>Contributing via Pull Requests</p> <p>Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that:</p> <ol> <li>You are working against the latest source on the main branch.</li> <li>You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.</li> <li>You open an issue to discuss any significant work - we would hate for your time to be wasted.</li> </ol> <p>To send us a pull request, please:</p> <ol> <li>Fork the repository.</li> <li>Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.</li> <li>Ensure local tests pass.</li> <li>Commit to your fork using clear commit messages.</li> <li>Send us a pull request, answering any default questions in the pull request interface.</li> <li>Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.</li> </ol> <p>GitHub provides additional document on forking a repository and creating a pull request.</p> <p>Finding contributions to work on</p> <p>Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.</p> <p>Code of Conduct</p> <p>This project has adopted the Amazon Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.</p> <p>Security issue notifications</p> <p>If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page. Please do not create a public github issue.</p> <p>Licensing</p> <p>See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.</p>"},{"location":"contributors/","title":"Contributors","text":"<p>The content on this site is maintained by the Solutions Architects and Technical Account Manager with support from the AWS service teams, and other volunteers from across the organization.</p> <p>Our goal is to make it easier to use AWS Native and Open Source Observability Services.</p> <p>The core team include the following people, in alphabetical order:</p> <ul> <li>KK (Krishna) Venkateswaran</li> <li>Mitesh Purohit</li> <li>Munish Dabra</li> <li>Mythili Annamalai Sekar</li> <li>Piyush Mattoo</li> <li>Prithvi Reddy</li> <li>Ravi Mathur</li> </ul> <p>We welcome the wider open source community to this project.</p> <p>Note that all information published on this site is available via the MIT-0 license.</p>"},{"location":"deployment/","title":"Deployment","text":"<p>This section demonstrates how you can leverage DAT to provision new cluster.</p>"},{"location":"deployment/#prerequisites","title":"Prerequisites","text":"<p>First, ensure that you have installed the following tools locally.</p> <ol> <li>AWS CLI</li> <li>Configure AWS CLI</li> <li>Terraform</li> </ol>"},{"location":"deployment/#deployment-options","title":"Deployment Options","text":"<p>DAT modules can be deployed using any one of the following options</p> <ol> <li> <p>Deployment Steps using new Jenkins</p> </li> <li> <p>Deployment Steps using existing Jenkins</p> </li> <li> <p>Deployment Steps using CLI</p> </li> </ol>"},{"location":"deployment_using_cli/","title":"Deployment Steps using CLI","text":"<p>The following steps will walk you through the deployment of <code>aurora-postgres-cluster-existing-vpc</code> example blueprint. This example expects you to leverage an existing VPC and provision a new Aurora Cluster with one writer and one reader instance. However you can customize the reader and writer instances:</p>"},{"location":"deployment_using_cli/#step-1-clone-the-repo-using-the-command-below","title":"Step 1: Clone the repo using the command below","text":"<pre><code>git clone https://github.com/aws-samples/aws-database-acceleration-toolkit.git\n</code></pre>"},{"location":"deployment_using_cli/#step-2-review-and-update-the-terraformtfvars","title":"Step 2: Review and update the terraform.tfvars","text":"<p>Navigate to <code>aurora-postgres-cluster-existing-vpc</code> under <code>aws-database-acceleration-toolkit/examples</code> folder. </p> <p>```shell script cd aws-database-acceleration-toolkit/examples/aurora-postgres-cluster-existing-vpc</p> <pre><code>Review the Terraform variable definition file called `terraform.tfvars` and update the values for the variables as per your use case. \n</code></pre>"},{"location":"deployment_using_cli/#aws-region-where-your-resources-will-be-located","title":"AWS Region where your resources will be located.","text":""},{"location":"deployment_using_cli/#for-example-us-west-2","title":"For example: \"us-west-2\"","text":"<p>region = \"\""},{"location":"deployment_using_cli/#vpc-id-where-your-resources-will-be-located","title":"VPC Id where your resources will be located.","text":""},{"location":"deployment_using_cli/#for-example-vpc-11112222333344445","title":"For example: \"vpc-11112222333344445\"","text":"<p>vpc_id = \"\""},{"location":"deployment_using_cli/#database-engine-for-your-aurora-mysql-cluster","title":"Database Engine for your Aurora mysql Cluster.","text":"<p>engine = \"aurora-postgresql\"</p>"},{"location":"deployment_using_cli/#database-engine-version-optional-if-not-specified-the-default-version-for-the-selected-engine-will-be-used","title":"Database engine version (optional). If not specified, the default version for the selected engine will be used.","text":""},{"location":"deployment_using_cli/#for-example-153","title":"For example: \"15.3\"","text":"<p>engine_version = \"\""},{"location":"deployment_using_cli/#database-engine-mode-valid-value-provisioned","title":"Database engine mode. Valid value: provisioned","text":""},{"location":"deployment_using_cli/#refer-aws-documentation-for-supported-regions-and-engine-versions-for-engine-mode","title":"Refer AWS documentation for supported regions and engine versions for engine mode","text":"<p>engine_mode = \"provisioned\"</p>"},{"location":"deployment_using_cli/#db-instance-class","title":"DB Instance class.","text":""},{"location":"deployment_using_cli/#refer-aws-documentation-for-supported-db-instance-class-for-db-engine","title":"Refer AWS documentation for supported DB instance class for DB engine.","text":""},{"location":"deployment_using_cli/#for-example-dbr6glarge","title":"For example: \"db.r6g.large\"","text":"<p>instance_class =\"\""},{"location":"deployment_using_cli/#specify-number-of-db-instances-to-be-created-in-the-cluster","title":"Specify number of DB instances to be created in the cluster.","text":""},{"location":"deployment_using_cli/#optionally-you-can-pass-the-configuration-parameters-and-values-for-eg-instance_classdbr6gxlarge-for-each-instance-within-the-curly-braces","title":"Optionally, you can pass the configuration parameters and values (for e.g., instance_class=\"db.r6g.xlarge\") for each instance within the curly braces.","text":""},{"location":"deployment_using_cli/#if-no-parameters-are-specified-all-the-db-instances-will-be-created-with-the-same-values","title":"If no parameters are specified, all the DB instances will be created with the same values.","text":"<p>instances = {     instance1   = {}     instance2   = {} }</p>"},{"location":"deployment_using_cli/#database-cluster-name","title":"Database cluster name","text":""},{"location":"deployment_using_cli/#for-example-aurora-pg-poc","title":"For example: \"aurora-pg-poc\"","text":"<p>name = \"\""},{"location":"deployment_using_cli/#database-environment","title":"Database environment","text":""},{"location":"deployment_using_cli/#for-example-dev","title":"For example: \"dev\"","text":"<p>environment = \"\""},{"location":"deployment_using_cli/#tagging-teamgroup-name","title":"Tagging : Team/Group Name","text":""},{"location":"deployment_using_cli/#for-example-data-engineering","title":"For example: \"data-engineering\"","text":"<p>groupname = \"\""},{"location":"deployment_using_cli/#tagging-project-or-application-name","title":"Tagging : Project or Application Name","text":""},{"location":"deployment_using_cli/#for-example-myapp","title":"For example: \"myapp\"","text":"<p>project = \"\""},{"location":"deployment_using_cli/#skip-final-snapshot-during-cluster-deletion-optional-if-set-to-true-default-no-final-snapshot-will-be-taken-before-deleting-the-cluster","title":"Skip final snapshot during cluster deletion (optional). If set to 'true' (default), no final snapshot will be taken before deleting the cluster.","text":"<p>skip_final_snapshot= \"true\"</p> <pre><code>The example below illustrates how to use the 'region' variable to define the AWS region for your database-related resources.\n```shell script\nregion = \"us-east-2\"\n</code></pre>"},{"location":"deployment_using_cli/#step-3-run-terraform-init","title":"Step 3: Run Terraform Init","text":"<p>Initialize a working directory with configuration files by running <code>terraform init</code> </p> <p>```shell script terraform init</p> <pre><code>### Step 4: Run Terraform Plan\nVerify the resources created by this execution using `terraform plan`\n\n```shell script\nterraform plan -var-file terraform.tfvars\n</code></pre>"},{"location":"deployment_using_cli/#step-5-terraform-apply","title":"Step 5: Terraform Apply","text":"<p>To create resources by running <code>terraform apply</code> commands</p> <p>```shell script terraform apply -var-file terraform.tfvars</p> <pre><code>### Cleanup: Terraform Destroy\n\nTo clean up your environment, destroy the AWS resources created \n\n```sh\nterraform destroy -var-file terraform.tfvars\n</code></pre>"},{"location":"deployment_using_existing_jenkins/","title":"Deployment Steps using Existing Jenkins","text":"<p>The following steps will walk you through the deployment of toolkit using your own Jenkins and running pipelines DAT modules. We assume that the basic installation of the Jenkins server is available either locally or on one of the virtual machines. If not, use Deployment Steps using new Jenkins deployment approach.</p> <ol> <li>Step 1 - Setup Job DSL Plugin in Jenkins</li> <li>Step 2 - Configure Seed Job</li> <li>Step 3 - Add AWS credentials in Jenkins</li> <li>Step 4 - Configure DAT pipelines for examples in Jenkins</li> <li>Step 5 - Run pipelines to deploy DAT modules</li> </ol>"},{"location":"deployment_using_existing_jenkins/#step-1-setup-job-dsl-plugin-in-jenkins","title":"Step 1: Setup Job DSL Plugin in Jenkins","text":"<p>Job DSL plugin is required to setup DAT pipelines. Validate if Jenkins already have it installed and enabled by going to Manage Jenkins -&gt; Plugins -&gt; Installed Plugins</p> <p></p> <p>To install DSL Plugin go to Manage Jenkins -&gt; Plugins -&gt; Available Plugins</p> <p>Search for \"Job DSL\" and install. Make sure plugin is enabled after installation</p>"},{"location":"deployment_using_existing_jenkins/#step-2-configure-seed-job","title":"Step 2: Configure Seed Job","text":"<p>Seed job is used to configure deployment piplelines.</p> <p>Browse Jenkins url and click \"New Item\" under Dashboard</p> <p>Enter an Item name as \"Seed Job\"</p> <p>Select \"Freestyle Project\" -&gt; Click Ok</p> <p></p> <p>Once redirected to Configure page enter below values</p> <p>Source Code Management: select git and enter \"https://github.com/aws-samples/aws-database-acceleration-toolkit\" in Repository URL and branch name as \"*/main\"</p> <p></p> <p>In Build section add build steps and select Process Job DSLs</p> <p></p> <p>Select checkbox \"Look on Filesystem\" and enter \"pipelines/seed_jobdsl.groovy\" value in DSL Scripts as shown below and click Save</p> <p></p>"},{"location":"deployment_using_existing_jenkins/#step-3-add-aws-credentials-in-jenkins","title":"Step 3: Add AWS credentials in Jenkins","text":"<ol> <li>Navigate to <code>Dashboard</code> -&gt; <code>Manage Jenkins</code> -&gt; <code>Credentials</code> -&gt; <code>System</code></li> </ol> <ol> <li>Navigate to <code>Global Credentials(unrestricted)</code> and click <code>Add credentials</code>.</li> <li>Enter AWS credentials of your environment, make sure you enter jenkinsaws in the ID field.</li> </ol>"},{"location":"deployment_using_existing_jenkins/#step-4-configure-dat-pipelines-for-examples-in-jenkins","title":"Step 4: Configure DAT pipelines for examples in Jenkins","text":"<ol> <li>Navigate to <code>Dashboard</code> in Jenkins. you will see <code>Seed job</code> pipeline created by default. It is used to setup <code>example</code> pipelines. </li> <li>Schedule a build for <code>Seed Job</code> by clicking green button. Job takes 2-5 seconds to complete and you will see addtional pipelines on the console.</li> </ol> <ol> <li>you will see additional pipelines for DAT examples on the console</li> </ol>"},{"location":"deployment_using_existing_jenkins/#step-5-run-pipelines-to-deploy-dat-modules","title":"Step 5: Run pipelines to deploy DAT modules","text":"<p>The below section describes steps for deploying <code>aurora-postgres-cluster-existing-vpc</code> module using pipelines.  The same steps are applicable for other modules also. </p>"},{"location":"deployment_using_existing_jenkins/#1-deploy-aurora-postgres-cluster-existing-vpc-module","title":"1. Deploy <code>aurora-postgres-cluster-existing-vpc</code> module","text":"<p>This aurora-postgres-cluster-existing-vpc example expects you to leverage an existing VPC and provision a new Aurora Cluster with one writer and one reader instance. However you can customize the reader and writer instances</p> <ol> <li>Navigate to <code>Dashboard</code> in Jenkins and select the <code>aurora-postgres-cluster-existing-vpc</code> module. Select <code>Buid with Parameters</code> option.</li> </ol> <p></p> <ol> <li>Enter the input parameters and click <code>Build</code></li> </ol> <p></p> <p><code>Note: It takes 12-15 minutes to setup RDS in your vpc. You can navigate to AWS console and search for RDS.</code> </p>"},{"location":"deployment_using_existing_jenkins/#2-destroy-aurora-postgres-cluster-existing-vpc-module","title":"2. Destroy <code>aurora-postgres-cluster-existing-vpc</code> module","text":"<ol> <li>Navigate to <code>Dashboard</code> in Jenkins and select the <code>aurora-postgres-cluster-existing-vpc</code> module. Select the Job that created the successful aurora cluster,</li> <li>Select <code>Rebuild</code> option and review the input parameters and make sure it reflects the aurora db cluster that you want destroy.</li> </ol> <ol> <li>Select <code>Destroy</code> checkbox and click <code>Rebuild</code></li> </ol> <p><code>Note: It takes 12-15 minutes to destroy RDS in your vpc. You can navigate to AWS console and verify</code> </p>"},{"location":"deployment_using_new_jenkins/","title":"Deployment Steps using New Jenkins Setup","text":"<p>The following steps will walk you through the deployment of toolkit using Jenkins and running pipelines DAT modules. </p> <ol> <li>Step 1 - Setup new Jenkins in Ec2</li> <li>Step 2 - Jenkins User Setup</li> <li>Step 3 - Configure Jenkins</li> <li>Step 4 - Run Jenkins pipelines and deploy DAT modules</li> <li>Step 5 - Destroy Jenkins</li> </ol>"},{"location":"deployment_using_new_jenkins/#step-1-setup-new-jenkins","title":"Step 1: Setup new Jenkins","text":"<p>The steps below guide you to create new EC2 instance in  VPC and install Jenkins on the docker in EC2.</p>"},{"location":"deployment_using_new_jenkins/#1-clone-dat-repository","title":"1. Clone DAT repository","text":"<pre><code>git clone https://github.com/aws-samples/aws-database-acceleration-toolkit.git\n</code></pre>"},{"location":"deployment_using_new_jenkins/#2-review-and-update-the-terraformtfvars","title":"2. Review and update the terraform.tfvars","text":"<p>Goto <code>aws-database-acceleration-toolkit</code> -&gt; <code>pipelines</code> -&gt; <code>Jenkins</code>  folder. </p> <p>```shell script cd aws-database-acceleration-toolkit/pipelines/Jenkins</p> <pre><code>Review the Terraform variable definition file called `terraform.tfvars` and update the values for the variables as per your use case. \n</code></pre>"},{"location":"deployment_using_new_jenkins/#mandatory-aws-region-where-your-resources-will-be-located","title":"(mandatory) AWS Region where your resources will be located","text":""},{"location":"deployment_using_new_jenkins/#for-example-us-west-2","title":"For example: \"us-west-2\"","text":"<p>region = \"&gt;\""},{"location":"deployment_using_new_jenkins/#mandatory-vpc-id-where-your-database-and-other-aws-resources-will-be-located","title":"(mandatory) VPC Id where your database and other AWS resources will be located.","text":""},{"location":"deployment_using_new_jenkins/#for-example-vpc-11112222333344445","title":"For example: \"vpc-11112222333344445\"","text":"<p>vpc_id = \"\" <pre><code>### 3. Run Terraform Init\nInitialize a working directory with configuration files by running `terraform init` \n\n```shell script\nterraform init\n</code></pre>"},{"location":"deployment_using_new_jenkins/#4-run-terraform-plan","title":"4. Run Terraform Plan","text":"<p>Verify the resources created by this execution using <code>terraform plan</code></p> <p>```shell script terraform plan -var-file terraform.tfvars</p> <pre><code>### 5. Terraform Apply to create Ec2 instance and setup Jenkins\nTo create resources by running `terraform apply` commands\n\n```shell script\nterraform apply -var-file terraform.tfvars\n</code></pre> <p>Once terraform apply is completed, console will show EC2 IP address as output. Save this IP address, we need this to login to jenkin console.</p> <p></p> <p><code>Note : It takes 2-3 minutes for Jenkins installation after terraform apply is complete. Wait till Status check is completed in EC2 console</code></p> <p></p>"},{"location":"deployment_using_new_jenkins/#step-2-unlock-jenkins-setup-user","title":"Step 2: Unlock Jenkins &amp; Setup user","text":"<ol> <li>Browse jenkin url http://[jenkins_ip]:8080 by replacing [jenkins_ip] with EC2 IP address copied in step 1. You will get the below Jenkins console </li> </ol> <ol> <li>To get the admin password, we need to login to Ec2 instance where Jenkin is deployed and run below commands    <code>shell script    sudo docker exec -it jenkins-docker bash    cat /var/jenkins_home/secrets/initialAdminPassword</code></li> </ol> <ol> <li>Copy the AdminPassword and enter into the Jenkins console and click <code>continue</code></li> <li>Select <code>Install suggested plugins</code> Option</li> <li>Complete First Admin User setup. </li> </ol> <ol> <li>Jenkin Initial setup is done</li> </ol>"},{"location":"deployment_using_new_jenkins/#step-3-configure-jenkins","title":"Step 3 - Configure Jenkins","text":""},{"location":"deployment_using_new_jenkins/#1-add-aws-credentials-in-jenkins","title":"1. Add AWS credentials in Jenkins","text":"<ol> <li>Login to (http://[jenkins_ip]:8080) using the admin user credentials </li> <li>Navigate to <code>Dashboard</code> -&gt; <code>Manage Jenkins</code> -&gt; <code>Credentials</code> -&gt; <code>System</code></li> </ol> <ol> <li>Navigate to <code>Global Credentials(unrestricted)</code> and click <code>Add credentials</code>.</li> <li>Enter AWS credentials of your environment, make sure you enter jenkinsaws in the ID field.</li> </ol>"},{"location":"deployment_using_new_jenkins/#2-configure-dat-pipelines-for-examples-in-jenkins","title":"2. Configure DAT pipelines for examples in Jenkins","text":"<ol> <li>Navigate to <code>Dashboard</code> in Jenkins. you will see <code>Seed job</code> pipeline created by default. It is used to setup <code>example</code> pipelines. </li> <li>Schedule a build for <code>Seed Job</code> by clicking green button. Job takes 2-5 seconds to complete and you will see addtional pipelines on the console.</li> </ol> <ol> <li>you will see additional pipelines for DAT examples on the console</li> </ol>"},{"location":"deployment_using_new_jenkins/#step-4-run-pipelines-to-deploy-dat-modules","title":"Step 4: Run pipelines to deploy DAT modules","text":"<p>The below section describes steps for deploying <code>aurora-postgres-cluster-existing-vpc</code> module using pipelines.  The same steps are applicable for other modules also. </p>"},{"location":"deployment_using_new_jenkins/#1-deploy-aurora-postgres-cluster-existing-vpc-module","title":"1. Deploy <code>aurora-postgres-cluster-existing-vpc</code> module","text":"<p>This aurora-postgres-cluster-existing-vpc example expects you to leverage an existing VPC and provision a new Aurora Cluster with one writer and one reader instance. However you can customize the reader and writer instances</p> <ol> <li>Navigate to <code>Dashboard</code> in Jenkins and select the <code>aurora-postgres-cluster-existing-vpc</code> module. Select <code>Buid with Parameters</code> option.</li> </ol> <p></p> <ol> <li>Enter the input parameters and click <code>Build</code></li> </ol> <p></p> <p><code>Note: It takes 12-15 minutes to setup RDS in your vpc. You can navigate to AWS console and search for RDS.</code> </p>"},{"location":"deployment_using_new_jenkins/#2-destroy-aurora-postgres-cluster-existing-vpc-module","title":"2. Destroy <code>aurora-postgres-cluster-existing-vpc</code> module","text":"<ol> <li>Navigate to <code>Dashboard</code> in Jenkins and select the <code>aurora-postgres-cluster-existing-vpc</code> module. Select the Job that created the successful aurora cluster,</li> <li>Select <code>Rebuild</code> option and review the input parameters and make sure it reflects the aurora db cluster that you want destroy.</li> </ol> <ol> <li>Select <code>Destroy</code> checkbox and click <code>Rebuild</code></li> </ol> <p><code>Note: It takes 12-15 minutes to destroy RDS in your vpc. You can navigate to AWS console and verify</code> </p>"},{"location":"deployment_using_new_jenkins/#step-5-destroy-jenkins","title":"Step 5: Destroy Jenkins","text":""},{"location":"deployment_using_new_jenkins/#cleanup-terraform-destroy-this-will-destroy-ec2-instance-and-jenkins","title":"Cleanup: Terraform Destroy - This will destroy Ec2 instance and Jenkins","text":"<p>To clean up your environment, destroy the AWS resources created </p> <p>Goto <code>aws-database-acceleration-toolkit</code> -&gt; <code>pipelines</code> -&gt; <code>Jenkins</code>  folder. </p> <p><code>shell script    cd aws-database-acceleration-toolkit/pipelines/Jenkins</code></p> <p>Run Terraform destroy </p> <p><code>sh    terraform destroy -var-file terraform.tfvars</code></p>"},{"location":"examples/","title":"Examples","text":"<p>Database Acceleration Toolkit comprises of below examples. Click each link to get more details on running these examples.</p> <ol> <li>aurora-mysql-cluster-existing-vpc - Creates new Amazon Aurora MySQL Cluster.</li> <li>aurora-mysql-cluster-global-db - Creates new Amazon Aurora MySQL Clusters in Primary and Secondary region.</li> <li>aurora-mysql-cluster-latest-snapshot - Deploys the Amazon Aurora MySQL Database Cluster from the latest cluster snapshot</li> <li>aurora-mysql-dbproxy - Creates proxy to existing Amazon Aurora MySQL.</li> <li>aurora-mysql-monitoring - Setup monitoring dashboard for Amazon Aurora MySQL.</li> <li>aurora-postgres-cluster-existing-vpc - Creates new Amazon Aurora PostgreSQL Cluster.</li> <li>aurora-postgres-cluster-global-db - Creates new Amazon Aurora PostgreSQL Clusters in Primary and Secondary region.</li> <li>aurora-postgres-cluster-latest-snapshot - Deploys the Amazon Aurora PostgreSQL Database Cluster from the latest cluster snapshot</li> <li>aurora-postgres-dbproxy - Creates proxy to existing Amazon Aurora PostgreSQL.</li> <li>aurora-postgres-monitoring - Setup monitoring dashboard for Amazon Aurora PostgreSQL.</li> </ol>"},{"location":"faq/","title":"FAQs","text":"<p>This section lists the Frequently Asked Questions</p> <p>Q: Is DAT an open-source solution?  Yes, DAT is an open-source solution that customers can use and customize according to their needs.</p> <p>Q: Who are the intended audiences for DAT?  The intended audiences for DAT are AWS customers who are migrating from commercial databases such as SQL Server to Amazon Aurora PostgreSQL.</p> <p>Q: Is there any cost associated with using DAT?  No, DAT is an open-source solution and is completely free to use. However, you will be responsible for any AWS costs associated with running your Aurora clusters and other AWS services.</p> <p>Q: What DB Engines are currently supported? Currently, we support PostgreSQL only. MySQL database engine is on the short-term roadmap.</p> <p>Q: How will customer deploy the solution? Customers will be able to download DAT as a IaC template from Github as one-click deployment.</p> <p>Q: Is there any community or support for DAT? DAT is supported by Solution Architects of AWS on best effort basis. However, users are encourged to ask questions, open issues, contribute and provide feedback on DAT.</p>"}]}